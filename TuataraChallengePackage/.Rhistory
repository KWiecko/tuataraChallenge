# 3. where the columns always have 0-1 values and those that have over 2 levels are split accordingly
# executing factorization
# case 1.
#
myTrnDataset1 <- simpleFactorizer(datasetToFactorize)
myTrnDatasetLog1 <- myTrnDataset1 %>% mutate(loss = log(loss))
# case 2.
#
myTrnDataset2 <- mapFactorizer(datasetToFactorize)
myTrnDatasetLog2 <- myTrnDataset2 %>% mutate(loss = log(loss))
# case 3.
#
myTrnDataset3 <- columnSplitterAndFactorizer(datasetToFactorize)
myTrnDatasetLog3 <- myTrnDataset3 %>% mutate(loss = log(loss))
listOfDatasetsToReturn <- list()
listOfDatasetsToReturn[[1]] <- list(myTrnDataset1, myTrnDatasetLog1)
listOfDatasetsToReturn[[2]] <- list(myTrnDataset2, myTrnDatasetLog2)
listOfDatasetsToReturn[[3]] <- list(myTrnDataset3, myTrnDatasetLog3)
names(listOfDatasetsToReturn) <- c("case1", "case2", "case3")
length(listOfDatasetsToReturn)
for(i in 1:length(listOfDatasetsToReturn)) {
names(listOfDatasetsToReturn[[i]]) <- c("myDataset", "myDatasetLog")
}
load_all()
load_all()
load_all()
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
load_all()
library(TuataraChallengePackage)
load_all()
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
# setting env varjust in case - may vary depending on system
Sys.setenv(SPARK_HOME = "/usr/local/spark")
#loading rquired packages
library(TuataraChallengePackage)
library(sparklyr)
library(dplyr)
#connecting
sc <- spark_connect(master = "yarn-client")
#loading data from kaggle to hdfs
#done via flume
#loading training dataset to hdfs
myTrnDataset <- spark_read_csv(sc, "myTrnData", "/konrad/tuatara/myTrnData/myTrnData")
#loading testing dataset to hdfs
myTstDataset <- spark_read_csv(sc, "myTstData", "/konrad/tuatara/myTstData/myTstData")
listToReturn <- list(sc, myTrnDataset, myTstDataset)
names(listToReturn) <- c("connectionObj", "myTrnDataset", "myTstDataset")
inputList <- listToReturn
setToFactorize <- "train"
#loading rquired packages
library(sparklyr)
library(dplyr)
library(TuataraChallengePackage)
if(setToFactorize == "train"){
datasetToFactorize <- collect(inputList[[2]])
}else{
datasetToFactorize <- collect(inputList[[3]])
}
# creating 3 factorized datasets:
# 1. where plain factor function is applied to every categorical column,
# 2. where there is a data sctucture conserved (f.e. A = 1, B = 2, ...),
# 3. where the columns always have 0-1 values and those that have over 2 levels are split accordingly
# creating preliminary dataset list to return to user
##
## list[[1]] Sparklyr connection object
##
## list[[2]] element: required (train or test) case 1. datasets:
##                                  - loss = loss
##                                  - loss = log(loss)
## list[[3]] element: required (train or test) case 2. datasets:
##                                  - loss = loss
##                                  - loss = log(loss)
## list[[4]] element: required (train or test) case 3. datasets:
##                                  - loss = loss
##                                  - loss = log(loss)
listOfDatasetsToReturn <- list()
# executing factorization
# case 1.
#
myTrnDataset1 <- simpleFactorizer(datasetToFactorize)
myTrnDatasetLog1 <- myTrnDataset1 %>% mutate(loss = log(loss))
listOfDatasetsToReturn[[1]] <- list(myTrnDataset1, myTrnDatasetLog1)
# case 2.
#
myTrnDataset2 <- mapFactorizer(datasetToFactorize)
myTrnDatasetLog2 <- myTrnDataset2 %>% mutate(loss = log(loss))
listOfDatasetsToReturn[[2]] <- list(myTrnDataset2, myTrnDatasetLog2)
# case 3.
#
myTrnDataset3 <- columnSplitterAndFactorizer(datasetToFactorize)
myTrnDatasetLog3 <- myTrnDataset3 %>% mutate(loss = log(loss))
listOfDatasetsToReturn[[3]] <- list(myTrnDataset3, myTrnDatasetLog3)
for(i in 1:length(listOfDatasetsToReturn)) {
names(listOfDatasetsToReturn[[i]]) <- c("myDataset", "myDatasetLog")
}
names(listOfDatasetsToReturn) <- c("case1", "case2", "case3")
dataFrameToSplit <- myTrnDataset1
columnToSplitBy <- dataFrameToSplit$loss
nOfRows = 10000
nOfRows = 20000
nOfSplits <- 1
library(caret)
library(dplyr)
pParam <- nOfRows/length(dataFrameToSplit) %>%
round(.2)
pParam
pParam <- nOfRows/length(dataFrameToSplit[,1]) %>%
round(.2)
pParam
length(dataFrameToSplit[,1])
pParam <- nOfRows/length(dataFrameToSplit[,1] %>% unlist( %>% as.vector())) %>%
round(.2)
pParam <- nOfRows/length(dataFrameToSplit[,1] %>% unlist() %>% as.vector())) %>%
round(.2)
pParam <- nOfRows/length(dataFrameToSplit[,1] %>% unlist() %>% as.vector()) %>%
round(.2)
pParam
pParam <- nOfRows/length(dataFrameToSplit[,1] %>% unlist() %>% as.vector()) %>%
round(.2)
pParam
pParam <- round(nOfRows/length(dataFrameToSplit[,1] %>% unlist() %>% as.vector()), 2)
pParam
pParam <- round(nOfRows/length(dataFrameToSplit[,1] %>% unlist() %>% as.vector()), 3)
pParam
subsetIdx <- createDataPartition(columnToSplitBy, p = pParam, k = nOfSplits)
subsetIdx <- createDataPartition(columnToSplitBy, p = pParam, times = nOfSplits)
class(subsetIdx)
subsetIdx$Resample1
listOfTrimmedDatasets <- lapply(subsetIdx, function(x){
return(dataFrameToSplit[x,])
})
nOfSplits <- 3
library(caret)
library(dplyr)
pParam <- round(nOfRows/length(dataFrameToSplit[,1] %>% unlist() %>% as.vector()), 3)
subsetIdx <- createDataPartition(columnToSplitBy, p = pParam, times = nOfSplits)
listOfTrimmedDatasets <- lapply(subsetIdx, function(x){
return(dataFrameToSplit[x,])
})
library(devtools)
library(roxygen2)
load_all()
load_all()
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
# setting env varjust in case - may vary depending on system
Sys.setenv(SPARK_HOME = "/usr/local/spark")
#loading rquired packages
library(TuataraChallengePackage)
library(sparklyr)
library(dplyr)
#connecting
sc <- spark_connect(master = "yarn-client")
#loading training dataset to hdfs
myTrnDataset <- spark_read_csv(sc, "myTrnData", "/konrad/tuatara/myTrnData/myTrnData")
#loading testing dataset to hdfs
myTstDataset <- spark_read_csv(sc, "myTstData", "/konrad/tuatara/myTstData/myTstData")
listToReturn <- list(sc, myTrnDataset, myTstDataset)
names(listToReturn) <- c("connectionObj", "myTrnDataset", "myTstDataset")
inputList <- listToReturn
setToFactorize <- "train"
library(sparklyr)
library(dplyr)
library(TuataraChallengePackage)
if(setToFactorize == "train"){
datasetToFactorize <- collect(inputList[[2]])
}else{
datasetToFactorize <- collect(inputList[[3]])
}
#loading rquired packages
library(TuataraChallengePackage)
library(sparklyr)
library(dplyr)
# getting columns to factorize
columnsToFactorize <- datasetToFactorize %>%
select(contains("cat")) %>%
colnames()
#finding all levels in all columns
uniqueDFLevels <- lapply(select(datasetToFactorize, contains("cat")), unique) %>%
unlist() %>%
as.vector() %>%
unique()
#mapping them to integers
uniqueDFLevelsMap <- as.integer(factor(uniqueDFLevels, labels = c(1:length(uniqueDFLevels))))
uniqueDFLevelsMapDF <- data.frame(uniqueDFLevels, uniqueDFLevelsMap)
colIdx <- 111
charsToBeMappedIdx <- grep(pattern = paste0("^",levels(factor(datasetToFactorize[ , colIdx] %>% unlist() %>% as.vector())), "$", collapse = "|"),
uniqueDFLevels)
replacementMap <- uniqueDFLevelsMapDF[charsToBeMappedIdx,]
mapIdx <- 1
source <- datasetToFactorize[ , colIdx] %>% unlist() %>% as.vector
class(source())
class(source
)
a <- gsub(pattern = replacementMap[mapIdx, 1],
replacement = replacementMap[mapIdx, 2],
dataSource)
dataSource <- datasetToFactorize[ , colIdx] %>% unlist() %>% as.vector
a <- gsub(pattern = replacementMap[mapIdx, 1],
replacement = replacementMap[mapIdx, 2],
dataSource)
a
for(mapIdx in 2:length(replacementMap[,1])){
#datasetToFactorize[ , colIdx]
a <- gsub(pattern = replacementMap[mapIdx, 1],
replacement = replacementMap[mapIdx, 2],
dataSource)
}
a
dataSource <- datasetToFactorize[ , colIdx] %>% unlist() %>% as.vector
for(mapIdx in 1:length(replacementMap[,1])){
#datasetToFactorize[ , colIdx]
dataSource <- gsub(pattern = replacementMap[mapIdx, 1],
replacement = replacementMap[mapIdx, 2],
dataSource)
}
dataSource
dataSource <- datasetToFactorize[ , colIdx] %>% unlist() %>% as.vector
for(mapIdx in 1:length(replacementMap[,1])){
#datasetToFactorize[ , colIdx]
dataSource <- gsub(pattern = paste0("^", replacementMap[mapIdx, 1], "$"),
replacement = replacementMap[mapIdx, 2],
dataSource)
}
dataSource
dataSource <- dataSource %>%
unlist() %>%
as.vector() %>%
as.integer()
dataSource
#loading rquired packages
library(TuataraChallengePackage)
library(sparklyr)
library(dplyr)
# getting columns to factorize
columnsToFactorize <- datasetToFactorize %>%
select(contains("cat")) %>%
colnames()
#finding all levels in all columns
uniqueDFLevels <- lapply(select(datasetToFactorize, contains("cat")), unique) %>%
unlist() %>%
as.vector() %>%
unique()
#mapping them to integers
uniqueDFLevelsMap <- as.integer(factor(uniqueDFLevels, labels = c(1:length(uniqueDFLevels))))
uniqueDFLevelsMapDF <- data.frame(uniqueDFLevels, uniqueDFLevelsMap)
for(colIdx in 2:(length(columnsToFactorize) + 1) ){
print(paste0("Dataset mapped and factorized in ", round(100 * colIdx/(length(columnsToFactorize) + 1), 2) , "%"))
charsToBeMappedIdx <- grep(pattern = paste0("^",levels(factor(datasetToFactorize[ , colIdx] %>% unlist() %>% as.vector())), "$", collapse = "|"),
uniqueDFLevels)
replacementMap <- uniqueDFLevelsMapDF[charsToBeMappedIdx,]
for(mapIdx in 1:length(replacementMap[,1])){
datasetToFactorize[ , colIdx]<- gsub(pattern = paste0("^", replacementMap[mapIdx, 1], "$"),
replacement = replacementMap[mapIdx, 2],
datasetToFactorize[ , colIdx] %>% unlist() %>% as.vector)
}
datasetToFactorize[ , colIdx]  <- datasetToFactorize[ , colIdx] %>%
unlist() %>%
as.vector() %>%
as.integer()
}
library(devtools)
library(roxygen2)
head(datasetToFactorize)
load_all()
library(TuataraChallengePackage)
load_all()
library(TuataraChallengePackage)
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
load_all()
library(TuataraChallengePackage)
is.element("dplyr", installed.packages()[,1])
installed.packages()
installed.packages()[,1]
class(installed.packages()[,1])
requiredPackages <- c("car", "nortest", "fBasics", "Hmisc",
"corrplot", "corrgram", "PerformanceAnalytics",
"jmuOutlier", "PairedData", "energy",
"corrr", "purrr", "fpc")
requiredPackages <- c("car", "nortest", "fBasics", "Hmisc",
"corrplot", "corrgram", "PerformanceAnalytics",
"jmuOutlier", "PairedData", "energy",
"corrr", "purrr", "fpc", "dplyr", "sparklyr")
for(package in requiredPackages) ifelse(grepl(pattern = "", installed.packages()[,1]),
,
install.packages(paste0(requiredPackages)))
requiredPackages <- c("car", "nortest", "fBasics", "Hmisc",
"corrplot", "corrgram", "PerformanceAnalytics",
"jmuOutlier", "PairedData", "energy",
"corrr", "purrr", "fpc", "dplyr", "sparklyr")
for(package in requiredPackages) ifelse(grepl(pattern = paste0(package), installed.packages()[,1]),
next,
install.packages(paste0(package)))
installed.packages()[,1]
for(package in requiredPackages) ifelse(grepl(pattern = paste0(package), installed.packages()[,1]),
next,
install.packages(paste0(package)))
grepl(pattern = paste0(package), installed.packages()[,1])
all(grepl(pattern = paste0(package), installed.packages()[,1])) == FALSE
!grepl(pattern = paste0(package), installed.packages()[,1])
all(grepl(pattern = paste0(package), installed.packages()[,1])) == FALSE
sum(grepl(pattern = paste0(package), installed.packages()[,1]))
requiredPackages <- c("car", "nortest", "fBasics", "Hmisc",
"corrplot", "corrgram", "PerformanceAnalytics",
"jmuOutlier", "PairedData", "energy",
"corrr", "purrr", "fpc", "dplyr", "sparklyr")
for(package in requiredPackages) ifelse(sum(grepl(pattern = paste0(package), installed.packages()[,1])) !=0,
next,
install.packages(paste0(package)))
requiredPackages <- c("car", "nortest", "fBasics", "Hmisc",
"corrplot", "corrgram", "PerformanceAnalytics",
"jmuOutlier", "PairedData", "energy",
"corrr", "purrr", "fpc", "dplyr", "sparklyr")
installedPacks <- installed.packages()[,1]
for(package in requiredPackages) {
if (sum(grepl(pattern = paste0(package), installedPacks)) !=0){
next
} else {
install.packages(paste0(package))
installedPacks <- installed.packages()[,1]
}
}
requiredPackages <- c("car", "nortest", "fBasics", "Hmisc",
"corrplot", "corrgram", "PerformanceAnalytics",
"jmuOutlier", "PairedData", "energy",
"corrr", "purrr", "fpc", "dplyr", "sparklyr")
installedPacks <- installed.packages()[,1]
for(package in requiredPackages) {
if (sum(grepl(pattern = paste0(package), installedPacks)) !=0){
next
} else {
install.packages(paste0(package))
installedPacks <- installed.packages()[,1]
}
}
library(devtools)
library(roxygen2)
load_All()
load_all()
library(TuataraChallengePackage)
# setting env varjust in case - may vary depending on system
Sys.setenv(SPARK_HOME = "/usr/local/spark")
#loading rquired packages
library(TuataraChallengePackage)
library(sparklyr)
library(dplyr)
#connecting
sc <- spark_connect(master = "yarn-client")
#loading data from kaggle to hdfs
#done via flume
#loading training dataset to hdfs
myTrnDataset <- spark_read_csv(sc, "myTrnData", "/konrad/tuatara/myTrnData/myTrnData")
#loading testing dataset to hdfs
myTstDataset <- spark_read_csv(sc, "myTstData", "/konrad/tuatara/myTstData/myTstData")
listToReturn <- list(sc, myTrnDataset, myTstDataset)
names(listToReturn) <- c("connectionObj", "myTrnDataset", "myTstDataset")
inputList <- listToReturn
setToFactorize <- "train"
caseOfFactorization <- "case1"
#loading rquired packages
library(sparklyr)
library(dplyr)
library(TuataraChallengePackage)
if(setToFactorize == "train"){
datasetToFactorize <- collect(inputList[[2]])
}else{
datasetToFactorize <- collect(inputList[[3]])
}
if(caseOfFactorization == "case1"){
myTrnDataset1 <- simpleFactorizer(datasetToFactorize)
myTrnDatasetLog1 <- myTrnDataset1 %>% mutate(loss = log(loss))
listOfDatasetsToReturn <- list(myTrnDataset1, myTrnDatasetLog1)
}
names(listOfDatasetsToReturn) <- c("myDataset", "myDatasetLog")
library(car)
library(rpub)
library(nortest)
library(fBasics)
library(corrplot)
library(corrgram)
library(PerformanceAnalytics)
library(jmuOutlier)
library(PairedData)
library(corrr)
library(energy)
library(purrr)
inputDataset <- myTrnDatasetLog1
inputDatasetMatrix <- inputDataset %>%
select(-id) %>%
as.matrix(inputDataset)
View(inputDataset)
inputDataset <- myTrnDatasetLog1
inputDatasetMatrix <- inputDataset %>%
select(-id)
library(dplyr)
inputDatasetMatrix <- inputDataset %>%
select(id)
View(inputDataset)
colnames(inputDataset)
inputDataset$id
inputDataset <- myTrnDatasetLog1
inputDatasetMatrix <- inputDataset %>%
select(id)
inputDatasetMatrix <- inputDataset[,-1]
inputDatasetMatrix <- inputDataset[,-1] %>%
as.matrix(inputDataset)
colnames(inputDatasetMatrix)
View(inputDatasetMatrix)
View(inputDatasetMatrix[,101:])
View(inputDatasetMatrix[,101:132])
View(inputDatasetMatrix[,101:131])
inputDataCorrMtx <-round( cor(inputDatasetMatrix,
method = "spearman"), 2)
class(inputDatasetMatrix)
inputDatasetMatrix[1:3, 1:3]
inputDataCorrMtx <-round( cor(inputDatasetMatrix,
method = "spearman"), 2)
inputDataCorrMtx
corrgram(inputDataCorrMtx,
method = "circle",
type = "lower",
bg = "wheat",
diag = F)
corrgram(cor(inputDatasetMatrix, method = "spearman"),
method = "circle",
type = "lower",
bg = "wheat",
diag = F)
corrgram(cor(inputDatasetMatrix),
method = "circle",
type = "lower",
bg = "wheat",
diag = F)
corrplot(cor(inputDatasetMatrix,  method = "spearman"),
method = "circle",
type = "lower",
bg = "wheat",
diag = F)
corrgram(inputDataCorrMtx,
lower.panel = panel.pie,
upper.panel = FALSE)
corrgram(inputDataCorrMtx,
lower.panel = panel.pie,
upper.panel = NULL)
colnames(inputDatasetMatrix)
?seq()
newVarNames <- gsub(pattern = "cont", replacement = "c", colnames(inputDatasetMatrix)) %>%
gsub(pattern = "cat", replacement = "", .)
newVarNames
# converting dataset to matrix
inputDatasetMatrix <- inputDataset[,-1] %>%
as.matrix(inputDataset)
newVarNames <- gsub(pattern = "cont", replacement = "c", colnames(inputDatasetMatrix)) %>%
gsub(pattern = "cat", replacement = "", .)
# changing colnames for sake of readibility
colnames(inputDatasetMatrix) <- newVarNames
inputDataCorrMtx <-round( cor(inputDatasetMatrix,
method = "spearman"), 2) # most of variables are categorical/do not have normal distribution
corrgram(inputDataCorrMtx,
lower.panel = panel.pie,
upper.panel = NULL)
load_all()
library(sparklyr)
spark_disconnect_all()
library(rpub)
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
?corr
?cor
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
library(devtools)
library(roxygen2)
load_all()
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
library(TuataraChallengePackage)
